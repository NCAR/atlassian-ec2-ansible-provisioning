#!/bin/bash

#
# FIXME ASSUME S3 URL LOOKS LIKE THIS:
#
# base_dir
#       - home
#       - database
#
# It's faster to sync an uncompressed home directory
# database directory should be as created by mysqlsh
#

APP="{{ atl_product_family }}"

s3_backup_url_raw=$1
if test -z "$s3_backup_url_raw"; then
       cat << EOF
Usage: $0 s3_backup_url

Restore database and home directory for app "$APP";
s3_backup_url must contain directories
- home      App home directory 
- backup    Backup as created by mysqlsh

THE APPLICATION SHOULD BE STOPPED BEFORE RESTORE!
EOF
exit 1
fi

# Lop off last char if it's /
s3_backup_url=$(echo $s3_backup_url_raw | sed -e 's/^\(.*\)\/$/\1/')

if systemctl is-active --quiet $APP; then
    echo "Service \"$APP\" is running; must be stopped before restore."
    exit 1
fi

if ! s5cmd ls "${s3_backup_url}/home" > /dev/null 2>&1; then
    echo "s3_backup_url doesn't have home directory"
    exit 1
fi

if ! s5cmd ls "${s3_backup_url}/database" > /dev/null 2>&1; then
    echo "s3_backup_url doesn't have database"
    exit 1
fi

# /var/tmp is bigger
export TMPDIR=/var/tmp

##############################################
# Restore home directory
##############################################
echo "Restoring home directory from clone backup"

# Tar up local retained files
# Filenames are for find command
retained_files=$(mktemp)
for pattern in $(yq -r '.filesystem.retained[]' /var/clone_config.yaml); do
    find -L {{ atl_product_home }} -type f -wholename "'${pattern'}" >> $retained_files
done
retained_files_tar=$(mktemp)
tar -C / --files-from=$retained_files -czf $retained_files_tar
rm $retained_files

# sync directory
s5cmd sync --delete "${s3_backup_url}/application/*" "{{ atl_product_home }}/"

# untar retained over sync'd directory
tar -C / -zxf $retained_files_tar
chown -R {{ atl_product_user }}:{{ atl_product_user }} {{ atl_product_home }}
rm $retained_files_tar

##############################################
# Restore database
##############################################
echo "Restoring database from clone backup"

# Save stuff we need from the database.  We do this by creating dumps
# for these by passing queries to mysqldump
mycnf=$(/usr/local/bin/create_mycnf.sh)
mysql_to_restore=$(mktemp -d)

# Whole tables
full_tables=$(yq -r '.database.retained.tables[]' /var/clone_config.yaml | tr "\n" " ")
mysqldump --defaults-file=$mycnf \
	  --add-drop-table \
	  --create-options \
	  --disable-keys \
	  --set-gtid-purged=OFF \
	  {{ atl_jdbc_db_name }} $full_tables > "${mysql_to_restore}/tables.sql" # Possible name collision?
# Queries
queried_tables=$(yq -r '.database.retained.queries | keys[]' /var/clone_config.yaml)
for table in $queried_tables; do
    for query in $(yq -r ".database.retained.queries.${table}[]" /var/clone_config.yaml); do
	mysqldump --defaults-file=$mycnf \
		  --disable-keys \
                  --no-create-info \
                  --no-tablespaces \
                  --replace \
                  --set-gtid-purged=OFF \
                  --single-transaction \
                  --skip-triggers \
		  --where="'${query}'" \
		  {{ atl_jdbc_db_name }} $table >> "${mysql_to_restore}/queried_tables.sql"
    done
done

# Parse s3_backup_url
bucketname=$(echo $s3_backup_url | cut -d '/' -f3)
prefix="$(echo $s3_backup_url | cut -d '/' -f4-)/database"

# Whatever version of the AWS API that mysqlsh is using, it needs
# credentials in env vars instead of the instance role
aws_env_vars=$(/usr/local/bin/aws_env_credentials_from_instance_role.sh)
source $aws_env_vars
rm $aws_env_vars

# Restore clone database over existing database
mysqlsh_cmd=$(mktemp)
cat << EOF >$mysqlsh_cmd
util.loadDump(
  "$prefix",
  {
    schema: "{{ atl_jdbc_db_name }}",
    s3BucketName: "$bucketname"
  }
)
EOF
mysqlsh --defaults-file=$mycnf --javascript --file=$mysqlsh_cmd
rm $mysqlsh_cmd

# Generate list of tables for literal string substitutions
edit_tables=$(mktemp)
for raw_table in $(yq -r ".database.substitutions.tables[]" /var/clone_config.yaml); do
    if echo $raw_table | grep -Fq '%'; then
	mysql --defaults-file=$mycnf --batch -e "show tables like '${table}';" >> $edit_tables
    else
	echo $raw_table >> $edit_tables
done

src_baseurl=$(/usr/local/bin/get_baseurl.sh)
src_site=$(echo $src_baseurl | awk -F/ '{print $3}')
dest_baseurl="{{ atl_base_url }}"
dest_site=$(echo $dest_baseurl | awk -F/ '{print $3}')

# Dump table, replace strings, restore table
tablesql=$(mktemp)
for table in $(cat $edit_tables); do
    mysqldump --defaults-file=$mycnf \
	      --disable-keys \
              --no-create-info \
              --no-tablespaces \
              --replace \
              --set-gtid-purged=OFF \
              --single-transaction \
              --skip-triggers \
	      $table > $tablesql

    # Replace base url, then replace site
    rpl -F "${src_baseurl@Q}" "${dest_baseurl@Q}" $tablesql
    rpl -F "${src_site@Q}" "${dest_site@Q}" $tablesql
    
    for subst_str in $(yq -r ".database.substitutions.strings[]" /var/clone_config.yaml); do
	# https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
	subst_val=$(yq -r ".database.substitutions.strings.${subst_str}" /var/clone_config.yaml)
	rpl -F "${subst_str@Q}" "${subst_val@Q}" $tablesql
    done
    
    mysql --defaults-file=$mycnf $table < $tablesql
    cat /dev/null > $tablesql
done
rm $tablesql
rm $edit_tables

# Restore edited and retained data
# Does the order matter here???
for f in $(ls "$mysql_to_restore/*"); do
    mysql --defaults-file=$mycnf < $f
done

# Delete log files since they're not relevant after clone
find . -wholename '*/logs/*' -delete -o -wholename '*/log/*' -delete

##############################################
# Postprocess
# Run app-specific post-process script if it exists
##############################################
test -e /usr/local/bin/clone_postprocess && /usr/local/bin/clone_postprocess

rm -rf $mysql_to_restore
rm $mycnf

