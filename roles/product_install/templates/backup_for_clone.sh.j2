#!/bin/bash

# This is used to crudely calculate whether we need to pass the
# --expected-size argument to "aws s3 cp" command. Since the wiki is
# probably the only site that needs this, I've derived from the
# compression ratio for kb.
TAR_COMPRESSION_RATIO=2.41

tag=$1
if test -z "$tag"; then
    echo "Usage: $0 tag"
    exit 1
fi

bucket_url="{{ atl_s3_path }}backups/$(date --iso-8601=seconds)-${tag}"

echo ">>> Backing up to $bucket_url"

bucketname=$(echo $bucket_url  | cut -d '/' -f3)

# Whatever version of the AWS API that mysqlsh is using, it needs
# credentials in env vars instead of the instance role
aws_env_vars=$(/usr/local/bin/aws_env_credentials_from_instance_role.sh)
source $aws_env_vars
rm $aws_env_vars

current_version=$(basename $(readlink -f /opt/atlassian/*/current))
echo $current_version | aws s3 cp - "${bucket_url}/VERSION"

#
# Backup plugin list
#
if systemctl is-active --quiet {{ atl_product_family }}; then
    if ! test -z $ATL_REST_TOKEN; then
	echo ">>> Backing up plugin list, starting $(date)"
	plugins_file=$(mktemp)
	/usr/local/bin/get_upm_plugins.py {{ atl_proxy_name }} > $plugins_file
	test $? -eq 0 || exit 1
	aws s3 cp $plugins_file "${bucket_url}/plugins.json"
	rm $plugins_file
    else
	echo ">>> Can't back up plugins because ATL_REST_TOKEN env var is not set; continuing anyway"
    fi
else
    echo ">>> Can't back up plugins because service isn't running so no access to REST API; continuing anyway"
fi

#
# DB backup
#
bucket_db_path="$(echo $bucket_url  | cut -d '/' -f4-)/database"
mysqlsh_cmd=$(mktemp)
cat << EOF >$mysqlsh_cmd
util.dumpSchemas(
  ["{{ atl_jdbc_db_name }}"],
  "$bucket_db_path",
  {
    s3BucketName: "$bucketname",
    threads: 8,
    compatibility: ["strip_definers"]
  }
)
EOF

mycnf=$(/usr/local/bin/create_mycnf.sh)

echo ">>> Backing up database to ${bucket_url}/database, starting $(date)"

mysqlsh --defaults-file=$mycnf --javascript --file=$mysqlsh_cmd
mysqlsh_status=$?
rm $mysqlsh_cmd
rm $mycnf

if [ "$mysqlsh_status" != "0" ]; then
    echo "Mysqlsh command failed"
    exit 1
fi

#
# FS backup; use tar to preserve symlinks
#
echo ">>> Backing up home filesystem, starting $(date)"

# Calculate an expected size, which is needed for streaming to "aws
# cp" if it's over 50GB. This is an *extremely* crude guess based on
# the ratio for kb.ucar.edu, since the only one over is probably
# wiki.ucar.edu.  I'm assuming that the --expected-size argument is
# ignored if it's under.          
homedir="{{ atl_product_home }}"
home_size=$(df --output=used -B 1 $homedir | grep -v Used)
compressed_home_size=$(echo "scale=0;$home_size/$TAR_COMPRESSION_RATIO" | bc -l)
echo "Expected size of home backup (for aws cp): $(echo "scale=2;$compressed_home_size/1073741824" | bc -l)GB"

tar --one-file-system \
    --directory $homedir \
    --verbose \
    --create \
    --zstd \
    --file - . | aws s3 cp - "${bucket_url}/home.zst" --expected-size $compressed_home_size


echo ">>> Finished at: " $(date)
echo "Total elapsed time: " $(printf '%dh:%dm:%ds\n' $(($SECONDS/3600)) $(($SECONDS%3600/60)) $((SECONDS%60)))
echo "S3 backup URL is: ${bucket_url}"

