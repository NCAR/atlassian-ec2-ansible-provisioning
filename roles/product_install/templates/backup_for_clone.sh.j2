#!/bin/bash

tag=$1
if test -z "$tag"; then
        cat << USAGE
$0 tag

Backup site for future restore or clone with restore_site.sh script.
If an admin user's is in the environment variable ATL_REST_TOKEN,
the script will back up plugins. 

USAGE
   exit 1
fi

bucket_url="{{ atl_s3_path }}backups/$(date --iso-8601=seconds)-${tag}"

echo ">>> Backing up to $bucket_url"

bucketname=$(echo $bucket_url  | cut -d '/' -f3)

# Whatever version of the AWS API that mysqlsh is using, it needs
# credentials in env vars instead of the instance role
aws_env_vars=$(/usr/local/bin/aws_env_credentials_from_instance_role.sh)
source $aws_env_vars
rm $aws_env_vars

current_version=$(basename $(readlink -f /opt/atlassian/*/current))
echo $current_version | aws s3 cp - "${bucket_url}/VERSION"

#
# Backup plugin list
#
if systemctl is-active --quiet {{ atl_product_family }}; then
    if ! test -z $ATL_REST_TOKEN; then
	echo ">>> Backing up plugin list, starting $(date)"
	plugins_file=$(mktemp)
	/usr/local/bin/get_upm_plugins.py {{ atl_proxy_name }} > $plugins_file
	test $? -eq 0 || exit 1
	aws s3 cp $plugins_file "${bucket_url}/plugins.json"
	rm $plugins_file
    else
	echo '{}' | aws s3 cp - "${bucket_url}/plugins.json"
	echo ">>> Can't back up plugins because ATL_REST_TOKEN env var is not set; continuing anyway"
    fi
else
    echo '{}' | aws s3 cp - "${bucket_url}/plugins.json"
    echo ">>> Can't back up plugins because service isn't running so no access to REST API; continuing anyway"
fi

#
# DB backup
#
bucket_db_path="$(echo $bucket_url  | cut -d '/' -f4-)/database"
mysqlsh_cmd=$(mktemp)
cat << EOF >$mysqlsh_cmd
util.dumpSchemas(
  ["{{ atl_jdbc_db_name }}"],
  "$bucket_db_path",
  {
    s3BucketName: "$bucketname",
    threads: 8,
    compatibility: ["strip_definers"]
  }
)
EOF

mycnf=$(/usr/local/bin/create_mycnf.sh)

echo ">>> Backing up database to ${bucket_url}/database, starting $(date)"

mysqlsh --defaults-file=$mycnf --javascript --file=$mysqlsh_cmd
mysqlsh_status=$?
rm $mysqlsh_cmd
rm $mycnf

if [ "$mysqlsh_status" != "0" ]; then
    echo "Mysqlsh command failed"
    exit 1
fi

#
# FS backup; use tar to preserve symlinks
#
echo ">>> Backing up home filesystem, starting $(date)"

# Get size of homedir in bytes; should be less when compressed so this is max
homedir="{{ atl_product_home }}"
home_size=$(df --output=used -B 1 $homedir | grep -v Used)
echo ">>> Expected size of home backup (for aws cp): ${home_size}"

tar --one-file-system \
    --directory $homedir \
    --verbose \
    --create \
    --use-compress-program='/usr/bin/zstd -T$(nproc)' \
    --file - . | aws s3 cp - "${bucket_url}/home.zst" --expected-size $compressed_home_size

echo ">>> Finished at: " $(date)
echo "Total elapsed time: " $(printf '%dh:%dm:%ds\n' $(($SECONDS/3600)) $(($SECONDS%3600/60)) $((SECONDS%60)))
echo "S3 backup URL is: ${bucket_url}"

